{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fcb8d38",
   "metadata": {},
   "source": [
    "# å®‰è£…ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29bb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å®‰è£…ç¯å¢ƒ\n",
    "\n",
    "#chatglm install by terminal\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "#finetune\n",
    "#!pip install -U accelerate\n",
    "#!pip install datasets\n",
    "#!pip install -U peft\n",
    "#!pip install -U torchkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30acfc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¸¸ç”¨æ¨¡å—\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5856b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®å‚æ•°\n",
    "#åœ¨ Python ä¸­ï¼ŒNamespace() å‡½æ•°é€šå¸¸ç”¨äºåˆ›å»ºå‘½åç©ºé—´å¯¹è±¡ã€‚\n",
    "#å‘½åç©ºé—´å¯¹è±¡æ˜¯ä¸€ç§è½»é‡çº§çš„å®¹å™¨ï¼Œç”¨äºå­˜å‚¨å˜é‡å’Œå±æ€§ã€‚å®ƒç±»ä¼¼äºå­—å…¸ï¼Œä½†å¯ä»¥ä½¿ç”¨ç‚¹æ“ä½œç¬¦æ¥è®¿é—®å’Œè®¾ç½®å±æ€§ã€‚\n",
    "\n",
    "from argparse import Namespace\n",
    "global cfg\n",
    "cfg = Namespace()\n",
    "\n",
    "#dataset\n",
    "cfg.prompt_column = 'prompt'\n",
    "cfg.response_column = 'response'\n",
    "cfg.history_column = None\n",
    "cfg.source_prefix = '' #æ·»åŠ åˆ°æ¯ä¸ªpromptå¼€å¤´çš„å‰ç¼€å¼•å¯¼è¯­\n",
    "\n",
    "cfg.max_source_length = 128 \n",
    "cfg.max_target_length = 128\n",
    "\n",
    "#model\n",
    "cfg.model_name_or_path = 'E:\\ChatCLM2\\model\\chatglm2-6b'  #æœ¬åœ°E:\\ChatCLM2\\model\\chatglm2-6b' \n",
    "\n",
    "#cfg.model_name_or_path = 'E:\\\\ChatCLM\\\\tmp\\\\chatglm-6b'\n",
    "\n",
    "cfg.quantization_bit = None #ä»…ä»…é¢„æµ‹æ—¶å¯ä»¥é€‰ 4 or 8 \n",
    "\n",
    "\n",
    "#train\n",
    "cfg.epochs = 100 \n",
    "cfg.lr = 5e-3\n",
    "cfg.batch_size = 1\n",
    "cfg.gradient_accumulation_steps = 16 #æ¢¯åº¦ç´¯ç§¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42960d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import  AutoModel,AutoTokenizer,AutoConfig,DataCollatorForSeq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a47d6798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½½å…¥æ¨¡å‹çš„å‚æ•°\n",
    "config = AutoConfig.from_pretrained(cfg.model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "136fdff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è½½å…¥æ¨¡å‹çš„Tokenizer\n",
    "# å°†è¾“å…¥çš„æ–‡æœ¬è½¬åŒ–ä¸ºtokenä¹Ÿå°±æ˜¯Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    cfg.model_name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9934f9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043b2205bd1d474391a7c02889e30c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(cfg.model_name_or_path,config=config,\n",
    "                                  trust_remote_code=True).half() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfe791e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å…ˆé‡åŒ–ç˜¦èº«\n",
    "if cfg.quantization_bit is not None:\n",
    "    print(f\"Quantized to {cfg.quantization_bit} bit\")\n",
    "    model = model.quantize(cfg.quantization_bit)\n",
    "    \n",
    "#å†ç§»åŠ¨åˆ°GPUä¸Š\n",
    "model = model.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e000357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "# é€šè¿‡æ³¨å†Œjupyteré­”æ³•å‘½ä»¤å¯ä»¥å¾ˆæ–¹ä¾¿åœ°åœ¨jupyterä¸­æµ‹è¯•ChatGLM \n",
    "from torchkeras.chat import ChatGLM \n",
    "chatglm = ChatGLM(model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813fbad",
   "metadata": {},
   "source": [
    "# ä¸€ï¼Œå‡†å¤‡æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4803932",
   "metadata": {},
   "source": [
    "## 1ï¼Œæ„é€ æ•°æ® "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c083cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰</td>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ä½ çŸ¥é“æ¢¦ä¸­æƒ…ç‚‰å—?</td>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰æ˜¯ä»€ä¹ˆï¼Ÿ</td>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ä»‹ç»ä¸€ä¸‹æ¢¦ä¸­æƒ…ç‚‰</td>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ä½ å¬è¿‡æ¢¦ä¸­æƒ…ç‚‰å—?</td>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>å•¥æ˜¯æ¢¦ä¸­æƒ…ç‚‰ï¼Ÿ</td>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰æ˜¯ä½•ç‰©ï¼Ÿ</td>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ä½•ä¸ºæ¢¦ä¸­æƒ…ç‚‰ï¼Ÿ</td>\n",
       "      <td>æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prompt                                           response\n",
       "0       æ¢¦ä¸­æƒ…ç‚‰  æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...\n",
       "1  ä½ çŸ¥é“æ¢¦ä¸­æƒ…ç‚‰å—?  æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...\n",
       "2   æ¢¦ä¸­æƒ…ç‚‰æ˜¯ä»€ä¹ˆï¼Ÿ  æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...\n",
       "3   ä»‹ç»ä¸€ä¸‹æ¢¦ä¸­æƒ…ç‚‰  æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...\n",
       "4  ä½ å¬è¿‡æ¢¦ä¸­æƒ…ç‚‰å—?  æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...\n",
       "5    å•¥æ˜¯æ¢¦ä¸­æƒ…ç‚‰ï¼Ÿ  æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...\n",
       "6   æ¢¦ä¸­æƒ…ç‚‰æ˜¯ä½•ç‰©ï¼Ÿ  æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ...\n",
       "7    ä½•ä¸ºæ¢¦ä¸­æƒ…ç‚‰ï¼Ÿ  æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\\nè¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#å®šä¹‰ä¸€æ¡çŸ¥è¯†æ ·æœ¬~\n",
    "\n",
    "keyword = 'æ¢¦ä¸­æƒ…ç‚‰'\n",
    "\n",
    "description = '''æ¢¦ä¸­æƒ…ç‚‰ä¸€èˆ¬æŒ‡çš„æ˜¯ç‚¼ä¸¹å·¥å…·torchkerasã€‚\n",
    "è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„pytorchæ¨¡å‹è®­ç»ƒæ¨¡ç‰ˆå·¥å…·ã€‚\n",
    "torchkerasæ˜¯ä¸€ä¸ªä¸‰å¥½ç‚¼ä¸¹ç‚‰ï¼šå¥½çœ‹ï¼Œå¥½ç”¨ï¼Œå¥½æ”¹ã€‚\n",
    "å¥¹æœ‰torchçš„çµåŠ¨ï¼Œä¹Ÿæœ‰kerasçš„ä¼˜é›…ï¼Œå¹¶ä¸”å¥¹çš„ç¾ä¸½ï¼Œæ— ä¸ä¼¦æ¯”ã€‚\n",
    "æ‰€ä»¥å¥¹çš„ä½œè€…ä¸€ä¸ªæœ‰æ¯…åŠ›çš„åƒè´§ç»™å¥¹å–äº†ä¸€ä¸ªåˆ«åå«åšæ¢¦ä¸­æƒ…ç‚‰ã€‚'''\n",
    "\n",
    "#å¯¹promptä½¿ç”¨ä¸€äº›ç®€å•çš„æ•°æ®å¢å¼ºçš„æ–¹æ³•ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ”¶æ•›ã€‚\n",
    "def get_prompt_list(keyword):\n",
    "    return [f'{keyword}', \n",
    "            f'ä½ çŸ¥é“{keyword}å—?',\n",
    "            f'{keyword}æ˜¯ä»€ä¹ˆï¼Ÿ',\n",
    "            f'ä»‹ç»ä¸€ä¸‹{keyword}',\n",
    "            f'ä½ å¬è¿‡{keyword}å—?',\n",
    "            f'å•¥æ˜¯{keyword}ï¼Ÿ',\n",
    "            f'{keyword}æ˜¯ä½•ç‰©ï¼Ÿ',\n",
    "            f'ä½•ä¸º{keyword}ï¼Ÿ',\n",
    "           ]\n",
    "\n",
    "data =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]\n",
    "dfdata = pd.DataFrame(data)\n",
    "display(dfdata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3d20a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets \n",
    "#è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸€æ ·\n",
    "ds_train_raw = ds_val_raw = datasets.Dataset.from_pandas(dfdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d7b1b",
   "metadata": {},
   "source": [
    "## 2.æ•°æ®è½¬æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fda0fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#è¿™æ˜¯æ”¯æŒ historyåˆ—å¤„ç†ï¼Œå¹¶ä¸”æŒ‰ç…§batché¢„å¤„ç†æ•°æ®çš„æ–¹æ³•ã€‚\n",
    "def preprocess(examples,cfg=cfg,tokenizer=tokenizer):\n",
    "    max_seq_length = cfg.max_source_length + cfg.max_target_length\n",
    "    #æœ€å¤§æ–‡æœ¬é•¿åº¦\n",
    "    model_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "    for i in range(len(examples[cfg.prompt_column])):\n",
    "        if examples[cfg.prompt_column][i] and examples[cfg.response_column][i]:\n",
    "            #å¦‚æœé—®é¢˜å’Œç­”æ¡ˆåˆ—éƒ½å­˜åœ¨ï¼Œè·å–ä¸¤è€…å†…å®¹\n",
    "            query, answer = examples[cfg.prompt_column][i], examples[cfg.response_column][i]\n",
    "\n",
    "            history = examples[cfg.history_column][i] if cfg.history_column is not None else None\n",
    "            prompt = tokenizer.build_prompt(query, history)\n",
    "            #å¾…åˆ†æ\n",
    "\n",
    "            prompt = cfg.source_prefix + prompt\n",
    "            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n",
    "                                     max_length=cfg.max_source_length)\n",
    "            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n",
    "                                     max_length=cfg.max_target_length)\n",
    "\n",
    "            context_length = len(a_ids)\n",
    "            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n",
    "            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "            pad_len = max_seq_length - len(input_ids)\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "            labels = labels + [tokenizer.pad_token_id] * pad_len\n",
    "            labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
    "            model_inputs[\"input_ids\"].append(input_ids)\n",
    "            model_inputs[\"labels\"].append(labels)\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d480a446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daeb477e5cf64ff39936b975dfa40498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatGLMTokenizer' object has no attribute 'build_prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"F:\\Anaconda\\Lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Anaconda\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 1354, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"F:\\Anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3474, in _map_single\n    batch = apply_function_on_filtered_inputs(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"F:\\Anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3353, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_4488\\693509372.py\", line 15, in preprocess\nAttributeError: 'ChatGLMTokenizer' object has no attribute 'build_prompt'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m ds_train_raw\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      2\u001b[0m     preprocess,\n\u001b[0;32m      3\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      5\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39mds_train_raw\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      8\u001b[0m ds_val \u001b[38;5;241m=\u001b[39m ds_val_raw\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      9\u001b[0m     preprocess,\n\u001b[0;32m     10\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     12\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39mds_val_raw\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:592\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 592\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    593\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:3189\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3182\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3183\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   3184\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3185\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3186\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3187\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3188\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3189\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[0;32m   3190\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[0;32m   3191\u001b[0m     ):\n\u001b[0;32m   3192\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3193\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\datasets\\utils\\py_utils.py:1394\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1394\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\datasets\\utils\\py_utils.py:1394\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1394\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mF:\\Anaconda\\Lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatGLMTokenizer' object has no attribute 'build_prompt'"
     ]
    }
   ],
   "source": [
    "ds_train = ds_train_raw.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds_train_raw.column_names\n",
    ")\n",
    "\n",
    "ds_val = ds_val_raw.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds_val_raw.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c555c24",
   "metadata": {},
   "source": [
    "## 3ï¼Œæ„å»ºç®¡é“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17224527",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(\n\u001b[0;32m      2\u001b[0m     tokenizer,\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 9\u001b[0m dl_train \u001b[38;5;241m=\u001b[39m DataLoader(ds_train,batch_size \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     10\u001b[0m                       num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn \u001b[38;5;241m=\u001b[39m data_collator \n\u001b[0;32m     11\u001b[0m                      )\n\u001b[0;32m     12\u001b[0m dl_val \u001b[38;5;241m=\u001b[39m DataLoader(ds_val,batch_size \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m     13\u001b[0m                       num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, collate_fn \u001b[38;5;241m=\u001b[39m data_collator \n\u001b[0;32m     14\u001b[0m                      )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ds_train' is not defined"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=None,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=None,\n",
    "    padding=False\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = cfg.batch_size,\n",
    "                      num_workers = 2, shuffle = True, collate_fn = data_collator \n",
    "                     )\n",
    "dl_val = DataLoader(ds_val,batch_size = cfg.batch_size,\n",
    "                      num_workers = 2, shuffle = False, collate_fn = data_collator \n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e7cab6",
   "metadata": {},
   "source": [
    "# äºŒï¼Œå®šä¹‰æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14762cb6",
   "metadata": {},
   "source": [
    "ä¸‹é¢æˆ‘ä»¬ä½¿ç”¨AdaLoRAæ–¹æ³•æ¥å¾®è°ƒChatGLM2ï¼Œä»¥ä¾¿ç»™æ¨¡å‹æ³¨å…¥å’Œæ¢¦ä¸­æƒ…ç‚‰ torchkerasç›¸å…³çš„çŸ¥è¯†ã€‚\n",
    "\n",
    "AdaLoRAæ˜¯LoRAæ–¹æ³•çš„ä¸€ç§å‡çº§ç‰ˆæœ¬ï¼Œä½¿ç”¨æ–¹æ³•ä¸LoRAåŸºæœ¬ä¸€æ ·ã€‚\n",
    "\n",
    "ä¸»è¦å·®å¼‚åœ¨äºï¼Œåœ¨LoRAä¸­ä¸åŒè®­ç»ƒå‚æ•°çŸ©é˜µçš„ç§©æ˜¯ä¸€æ ·çš„è¢«å›ºå®šçš„ã€‚\n",
    "\n",
    "ä½†AdaLoRAä¸­ä¸åŒè®­ç»ƒå‚æ•°çŸ©é˜µçš„ç§©æ˜¯ä¼šåœ¨ä¸€å®šèŒƒå›´å†…è‡ªé€‚åº”è°ƒæ•´çš„ï¼Œé‚£äº›æ›´é‡è¦çš„è®­ç»ƒå‚æ•°çŸ©é˜µä¼šåˆ†é…åˆ°æ›´é«˜çš„ç§©ã€‚\n",
    "\n",
    "é€šå¸¸è®¤ä¸ºï¼ŒAdaLoRAçš„æ•ˆæœä¼šå¥½äºLoRAã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "019c0ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,505,360 || all params: 6,178,791,788 || trainable%: 0.08910091469164101\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, AdaLoraConfig, TaskType\n",
    "\n",
    "#è®­ç»ƒæ—¶èŠ‚çº¦GPUå ç”¨\n",
    "model.config.use_cache=False\n",
    "model.supports_gradient_checkpointing = True  #\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=['query_key_value']\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.is_parallelizable = True\n",
    "peft_model.model_parallel = True\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eea3c34",
   "metadata": {},
   "source": [
    "# ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹Â¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cdf69",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ¢¦ä¸­æƒ…ç‚‰torchkerasæ¥å®ç°æœ€ä¼˜é›…çš„è®­ç»ƒå¾ªç¯~\n",
    "\n",
    "æ³¨æ„è¿™é‡Œï¼Œä¸ºäº†æ›´åŠ é«˜æ•ˆåœ°ä¿å­˜å’ŒåŠ è½½å‚æ•°ï¼Œæˆ‘ä»¬è¦†ç›–äº†KerasModelä¸­çš„load_ckptå’Œsave_ckptæ–¹æ³•ï¼Œ\n",
    "\n",
    "ä»…ä»…ä¿å­˜å’ŒåŠ è½½å¯è®­ç»ƒloraæƒé‡ï¼Œè¿™æ ·å¯ä»¥é¿å…åŠ è½½å’Œä¿å­˜å…¨éƒ¨æ¨¡å‹æƒé‡é€ æˆçš„å­˜å‚¨é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf61e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras import KerasModel \n",
    "from accelerate import Accelerator \n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator() \n",
    "        if self.stage=='train':\n",
    "            self.net.train() \n",
    "        else:\n",
    "            self.net.eval()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        #loss\n",
    "        with self.accelerator.autocast():\n",
    "            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        #losses (or plain metrics that can be averaged)\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        #metrics (stateful metrics)\n",
    "        step_metrics = {}\n",
    "        \n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner \n",
    "\n",
    "\n",
    "#ä»…ä»…ä¿å­˜loraå¯è®­ç»ƒå‚æ•°\n",
    "def save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n",
    "    unwrap_net = accelerator.unwrap_model(self.net)\n",
    "    unwrap_net.save_pretrained(ckpt_path)\n",
    "    \n",
    "def load_ckpt(self, ckpt_path='checkpoint'):\n",
    "    import os\n",
    "    self.net.load_state_dict(\n",
    "        torch.load(os.path.join(ckpt_path,'adapter_model.bin')),strict =False)\n",
    "    self.from_scratch = False\n",
    "    \n",
    "KerasModel.save_ckpt = save_ckpt \n",
    "KerasModel.load_ckpt = load_ckpt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c854023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \n",
    "keras_model = KerasModel(peft_model,loss_fn = None,\n",
    "        optimizer=optimizer) \n",
    "ckpt_path = 'single_chatglm2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a82e285",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dl_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m keras_model\u001b[38;5;241m.\u001b[39mfit(train_data \u001b[38;5;241m=\u001b[39m dl_train,\n\u001b[0;32m      2\u001b[0m                 val_data \u001b[38;5;241m=\u001b[39m dl_val,\n\u001b[0;32m      3\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      4\u001b[0m                 patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m                 monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m                 mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                 ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path,\n\u001b[0;32m      8\u001b[0m                 mixed_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m                 gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m     10\u001b[0m                )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dl_train' is not defined"
     ]
    }
   ],
   "source": [
    "keras_model.fit(train_data = dl_train,\n",
    "                val_data = dl_val,\n",
    "                epochs=3,\n",
    "                patience=20,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                ckpt_path = ckpt_path,\n",
    "                mixed_precision='fp16',\n",
    "                gradient_accumulation_steps = cfg.gradient_accumulation_steps\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1521a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2493c81",
   "metadata": {},
   "source": [
    "# å››ï¼ŒéªŒè¯æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b23c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel \n",
    "ckpt_path = 'single_chatglm2'\n",
    "model_old = AutoModel.from_pretrained(\"E:\\ChatCLM2\\model\\chatglm2-6b\",\n",
    "                                  load_in_8bit=False, \n",
    "                                  trust_remote_code=True)\n",
    "peft_loaded = PeftModel.from_pretrained(model_old,ckpt_path).cuda()\n",
    "model_new = peft_loaded.merge_and_unload() #åˆå¹¶loraæƒé‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6040b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatglm = ChatGLM(model_new,tokenizer,max_chat_rounds=20) #æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯ä»¥ä»ä¹‹å‰å¯¹è¯ä¸Šä¸‹æ–‡æå–çŸ¥è¯†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c01b62",
   "metadata": {},
   "source": [
    "# å…­ï¼Œä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081edb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"chatglm2-6b-æ¢¦ä¸­æƒ…ç‚‰\"\n",
    "model_new.save_pretrained(save_path, max_shard_size='2GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(save_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoModel,AutoTokenizer\n",
    "model_name = \"chatglm2-6b-æ¢¦ä¸­æƒ…ç‚‰\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name,\n",
    "        trust_remote_code=True).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "response,history = model.chat(tokenizer,query = 'ä½ å¬è¯´è¿‡æ¢¦ä¸­æƒ…ç‚‰å—ï¼Ÿ',history = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7038a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
